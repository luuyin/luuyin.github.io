---
---




@inproceedings{he2025alphadecay,
  abbr={NeurIPS 2025},
  title={AlphaDecay: Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs},
  booktitle={NeurIPS 2025},
  author={He, Di and Jaiswal, Ajay and Tu, Songjun and Shen, Li and Yuan, Ganzhao and Liu, Shiwei and <b>Yin, Lu</b>&dagger; (<b>&dagger; corresponding author</b>)},
  year={2025},
  selected={true},
  html={https://arxiv.org/abs/2506.14562},
  code={https://github.com/hed-ucas/AlphaDecay}
}


@inproceedings{he2025coe,
  abbr={NeurIPS 2025},
  title={The Curse of Depth in Large Language Models},
  booktitle={NeurIPS 2025},
  author={Sun, Wenfang and Song, Xinyuan and Li, Pengxiang and <b>Yin, Lu</b> and Zheng, Yefeng and Liu, Shiwei},
  year={2025},
  selected={true},
  html={https://arxiv.org/pdf/2502.05795},
  code={https://github.com/lmsdss/LayerNorm-Scaling}
}


@inproceedings{he2025gpas,
  abbr={NeurIPS 2025},
  title={GPAS: Accelerating Convergence of LLM Pretraining via Gradient-Preserving Activation Scaling},
  author={Chen, Tianhao and Xu, Xin and Liu, Zijing and Li, Pengxiang and Song, Xinyuan and Jaiswal, Ajay Kumar and Zhang, Fan and Hu, Jishan and Wang, Yang and Chen, Hao and  <b>Yin, Lu</b>&dagger and Yang, Can (<b>&dagger; corresponding author</b>)},
  year={2025},
  selected={true},
  html={https://arxiv.org/abs/2506.22049},
  code={https://github.com/dandingsky/GPAS}
}




@inproceedings{yin2023pruning,
  abbr={ICLR 2025},
  title={Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and Post-LN},
  author={Li, Pengxiang$*$ and <b>Yin, Lu</b>$*$ and Liu, Shiwei ($*$ equal contribution)},
  html={https://arxiv.org/abs/2412.13795},
  code={https://github.com/pixeli99/MixLN},
  selected={true},
  year={2025}
  }


@inproceedings{yin2023OWL,
  abbr={ICML 2024},
  title={Outlier Weighed Layerwise Sparsity OWL: A Missing Secret Sauce for Pruning LLMs to High Sparsity},
  author={<b>Yin, Lu</b> and You, Wu and Zhenyu, Zhang and Cheng-Yu, Hsieh and Yaqing, Wang and Yiling, Jia and Mykola, Pechenizkiy and Yi, Liang and Zhangyang, Wang and Shiwei, Liu},
  journal={Phys. Rev.,},
  html={https://arxiv.org/pdf/2310.05175.pdf},
  code={https://github.com/luuyin/OWL},
  selected={true},
}

@inproceedings{yin2023pruning,
  abbr={ICML 2024},
  title={Junk DNA Hypothesis: Pruning Small Pre-Trained Weights Irreversibly and Monotonically Impairs "Difficult" Downstream Tasks in LLMs},
  author={<b>Yin, Lu</b> and Jaiswal, Ajay and Liu, Shiwei and Kundu, Souvik and Wang, Zhangyang},
  html={https://arxiv.org/pdf/2310.02277},
  code={https://github.com/VITA-Group/Junk_DNA_Hypothesis},
  selected={true},
  }


@inproceedings{he2025alphadecay,
  abbr={NeurIPS 2023},
  title={Dynamic Sparsity is Channel-level Sparsity Learner},
  author={<b>Yin, Lu</b> and Li, Gen and Fang, Meng and Shen, Li and Huang, Tianjin and Wang, Zhangyang and Menkovski, Vlado and Ma, Xiaolong and Pechenizkiy, Mykola and Liu, Shiwei and others},
  booktitle={NeurIPS 2023},
  year={2023},
  selected={true},
  code={https://github.com/luuyin/chase},
  html={https://proceedings.neurips.cc/paper_files/paper/2023/file/d6d0e41e0b1ed38c76d13c9e417a8f1f-Paper-Conference.pdf}
}




@inproceedings{yin2022lottery,
  abbr={AAAI 2022},
  title={Lottery Pools: Winning More by Interpolating Tickets without Increasing Training or Inference Cost},
  author={<b>Yin, Lu</b> and Liu, Shiwei and Fang, Meng and Huang, Tianjin and Menkovski, Vlado and and Pechenizkiy},
   booktitle={Thirty-Seventh AAAI Conference on Artificial Intelligence},
  journal={Phys. Rev.,},
  year={2023},
  html={https://arxiv.org/pdf/2208.10842.pdf},
  code={https://github.com/luuyin/Lottery-pools.},
  selected={true}
}


@inproceedings{huang2022you,
  abbr={LoG (Best Paper)},
  title={You Can Have Better Graph Neural Networks by Not Training Weights at All: Finding Untrained GNNs Tickets},
  booktitle={Learning on Graphs Conference},
  author={Tianjin Huang and Tianlong Chen and Meng Fang and Vlado Menkovski and Jiaxu Zhao and <b>Yin, Lu</b> and Yulong Pei,  Decebal Constantin Mocanu and Zhangyang Wang and Mykola Pechenizkiy and Shiwei Liu},
  abstract={Recent works have impressively demonstrated that there exists a subnetwork in randomly initialized convolutional neural networks (CNNs) that can match the performance of the fully trained dense networks at initialization, without any optimization of the weights of the network (i.e., untrained networks). However, the presence of such untrained subnetworks in graph neural networks (GNNs) still remains mysterious. In this paper we carry out the first-of-its-kind exploration of discovering matching untrained GNNs. With sparsity as the core tool, we can find untrained sparse subnetworks at the initialization, that can match the performance of fully trained dense GNNs. Besides this already encouraging finding of comparable performance, we show that the found untrained subnetworks can substantially mitigate the GNN over-smoothing problem, hence becoming a powerful tool to enable deeper GNNs without bells and whistles. We also observe that such sparse untrained subnetworks have appealing performance in out-of-distribution detection and robustness of input perturbations. We evaluate our method across widely-used GNN architectures on various popular datasets including the Open Graph Benchmark (OGB).},
  year={2022},
  html={https://openreview.net/forum?id=dF6aEW3_62O},
  pdf={Untrain_GNNs_Tickets.pdf},
}


@inproceedings{yin2022superposing,
  abbr={UAI},
  title={Superposing Many Tickets into One: A Performance Booster for Sparse Neural Network Training},
  author={<b>Yin, Lu</b> and Menkovski, Vlado and Fang, Meng and Huang, Tianjin and Pei, Yulong and Pechenizkiy, Mykola and Mocanu, Decebal Constantin and Liu, Shiwei},
  booktitle={The 38th Conference on Uncertainty in Artificial Intelligence},
  abstract={Recent works on sparse neural network training (sparse training) have shown that a compelling trade-off between performance and efficiency can be achieved by training intrinsically sparse neural networks from scratch. Existing sparse training methods usually strive to find the best sparse subnetwork possible in one single run, without involving any expensive dense or pre-training steps. For instance, dynamic sparse training (DST), as one of the most prominent directions,  is capable of reaching a competitive performance of dense training by iteratively evolving the sparse topology during the course of training. In this paper, we argue that it is better to allocate the limited resources to create multiple low-loss sparse subnetworks and superpose them into a stronger one, instead of allocating all resources entirely to find an individual subnetwork. To achieve this, two desiderata are required: (1) efficiently producing many low-loss subnetworks, the so-called cheap tickets, within one training process limited to the standard training time used in dense training; (2) effectively superposing these cheap tickets into one stronger subnetwork without going over the constrained parameter budget. To corroborate our conjecture, we present a novel sparse training approach, termed Sup-tickets, which can satisfy the above two desiderata concurrently in a single sparse-to-sparse training process. Across various modern architectures on CIFAR-10/100 and ImageNet, we show that Sup-tickets integrates seamlessly with the existing sparse training methods and demonstrates consistent performance improvement. },
  year={2022},
  html={https://openreview.net/forum?id=HeZlJPLoqgq},
  pdf={Superposing_Many_Tickets_into_One__UAI.pdf},
}





@inproceedings{liu2021we,
  abbr={ICML},
  title={Do we actually need dense over-parameterization? in-time over-parameterization in sparse training},
  author={Liu, Shiwei and <b>Yin, Lu</b> and Mocanu, Decebal Constantin and Pechenizkiy, Mykola},
  abstract={In this paper, we introduce a new perspective on training deep neural networks capable of state-of-the-art performance without the need for the expensive over-parameterization by proposing the concept of In-Time Over-Parameterization (ITOP) in sparse training. By starting from a random sparse network and continuously exploring sparse connectivities during training, we can perform an Over-Parameterization over the course of training, closing the gap in the expressibility between sparse training and dense training. We further use ITOP to understand the underlying mechanism of Dynamic Sparse Training (DST) and discover that the benefits of DST come from its ability to consider across time all possible parameters when searching for the optimal sparse connectivity. As long as sufficient parameters have been reliably explored, DST can outperform the dense neural network by a large margin. We present a series of experiments to support our conjecture and achieve the state-of-the-art sparse training performance with ResNet-50 on ImageNet. More impressively, ITOP achieves dominant performance over the overparameterization-based sparse methods at extreme sparsities. When trained with ResNet-34 on CIFAR-100, ITOP can match the performance of the dense model at an extreme sparsity 98%.},
  booktitle={International Conference on Machine Learning},
  pages={6989--7000},
  year={2021},
  organization={PMLR},
  html={http://proceedings.mlr.press/v139/liu21y/liu21y.pdf},
  pdf={Do We Actually Need Dense Over-Parameterization.pdf},
}


@article{liu2021sparse,
  title={Sparse training via boosting pruning plasticity with neuroregeneration},
  abbr={NeurIPS},
  author={Liu, Shiwei and Chen, Tianlong and Chen, Xiaohan and Atashgahi, Zahra and <b>Yin, Lu</b> and Kou, Huanyu and Shen, Li and Pechenizkiy, Mykola and Wang, Zhangyang and Mocanu, Decebal Constantin},
  journal={Advances in Neural Information Processing Systems},
  abstract={Works on lottery ticket hypothesis (LTH) and single-shot network pruning (SNIP) have raised a lot of attention currently on post-training pruning (iterative magnitude pruning) and before-training pruning (pruning at initialization). The former method suffers from an extremely large computation cost and the latter category of methods usually struggles with insufficient performance. In comparison, during-training pruning, a class of pruning methods that simultaneously enjoys the training/inference efficiency and the comparable performance, temporarily, has been less explored. To better understand during-training pruning, we quantitatively study the effect of pruning throughout training from the perspective of pruning plasticity (the ability of the pruned networks to recover the original performance). Pruning plasticity can help explain several other empirical observations about neural network pruning in literature. We further find that pruning plasticity can be substantially improved by injecting a brain-inspired mechanism called neuroregeneration, i.e., to regenerate the same number of connections as pruned. We design a novel gradual magnitude pruning (GMP) method, named gradual pruning with zero-cost neuroregeneration, GraNet, advancing state of the art. Perhaps most impressively, GraNet for the first time boosts the sparse-to-sparse training performance over various dense-to-sparse methods by a large margin with ResNet-50 on ImageNet without extending the training time. },
  html={https://openreview.net/forum?id=MNVjrDpu6Yo},
  pdf={Sparse training via boosting pruning plasticity with neuroregeneration.pdf},
  volume={34},
  year={2021}
}


@article{yin2021hierarchical,
abbr={AAAI (Workshop)},
abstract={Few-shot classification tasks aim to classify images in query sets based on only a few labeled examples in support sets.  Most studies usually assume that each image in a task has a single and unique class association. Under these assumptions, these algorithms may not be able to identify the proper class assignment when there is no exact matching between support and query classes.  For example, given a few images of lions, bikes, and apples to classify a tiger.  However, in a more general setting, we could consider the higher-level concept, the large carnivores, to match the tiger to the lion for semantic classification.  Existing studies rarely considered this situation due to the incompatibility of label-based supervision with complex conception relationships.  In this work,  we advance the few-shot learning towards this more challenging scenario, the semantic-based few-shot learning, and propose a method to address the paradigm by capturing the inner semantic relationships using interactive psychometric learning. The experiment results on the CIFAR-100 dataset show the superiority of our method for the semantic-based few-shot learning compared to the baseline.},
  title={Semantic-Based Few-Shot Learning by Interactive Psychometric Testing},
  author={<b>Yin, Lu</b> and Menkovski, Vlado and Yulong Pei and Pechenizkiy, Mykola},
  journal={AAAI 2022 Workshop on Interactive Machine Learning (IML@AAAI22)},
  year={2021},
  html={https://arxiv.org/abs/2112.09201},
  pdf={Semantic-Based Few-Shot Learning by Interactive Psychometric Testing.pdf},

}



@article{yin2021hierarchical,
abbr={ACML (Long Oral)},
abstract={Assigning meaning to parts of image data is the goal of semantic image segmentation. Machine learning methods, specifically supervised learning is commonly used in a variety of tasks formulated as semantic segmentation. One of the major challenges in the supervised learning approaches is expressing and collecting the rich knowledge that experts have with respect to the meaning present in the image data. Towards this, typically a fixed set of labels is specified and experts are tasked with annotating the pixels, patches or segments in the images with the given labels. In general, however, the set of classes does not fully capture the rich semantic information present in the images. For example, in medical imaging such as histology images, the different parts of cells could be grouped and sub-grouped based on the expertise of the pathologist. To achieve such a precise semantic representation of the concepts in the image, we need access to the full depth of knowledge of the annotator. In this work, we develop a novel approach to collect segmentation annotations from experts based on psychometric testing. Our method consists of the psychometric testing procedure, active query selection,  query enhancement, and a deep metric learning model to achieve a patch-level image embedding that allows for semantic segmentation of images. We show the merits of our method with evaluation on the synthetically generated image, aerial image and histology image.},
  title={Hierarchical Semantic Segmentation using Psychometric Learning},
  author={<b>Yin, Lu</b> and Menkovski, Vlado and Liu, Shiwei and Pechenizkiy, Mykola},
  journal={Proceedings of Machine Learning Research},
  year={2021},
  html={https://proceedings.mlr.press/v157/yin21a/yin21a.pdf},
  pdf={Hierarchical Semantic Segmentation using Psychometric Learnin.pdf}

}



@inproceedings{yin2021beyond,
abbr={IJCAI (DC)},
  title={Beyond labels: knowledge elicitation using deep metric learning and psychometric testing},
  author={<b>Yin, Lu</b>},
  booktitle={Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence},
  pages={5214--5215},
  year={2021},
  html={https://www.ijcai.org/proceedings/2020/0747.pdf},
}



@inproceedings{yin2020knowledge,
  abbr={ECML},
  abstract={Knowledge present in a domain is well expressed as relationships between corresponding concepts. For example, in zoology, animal species form complex hierarchies; in genomics, the different (parts of) molecules are organized in groups and subgroups based on their functions; plants, molecules, and astronomical objects all form complex taxonomies. Nevertheless, when applying supervised machine learning (ML) in such domains, we commonly reduce the complex and rich knowledge to a fixed set of labels, and induce a model shows good generalization performance with respect to these labels. The main reason for such a reductionist approach is the difficulty in eliciting the domain knowledge from the experts. Developing a label structure with sufficient fidelity and providing comprehensive multi-label annotation can be exceedingly labor-intensive in many real-world applications. In this paper, we provide a method for efficient hierarchical knowledge elicitation (HKE) from experts working with high-dimensional data such as images or videos. Our method is based on psychometric testing and active deep metric learning. The developed models embed the high-dimensional data in a metric space where distances are semantically meaningful, and the data can be organized in a hierarchical structure. We provide empirical evidence with a series of experiments on a synthetically generated dataset of simple shapes, and Cifar 10 and Fashion-MNIST benchmarks that our method is indeed successful in uncovering hierarchical structures.},
  title={Knowledge Elicitation Using Deep Metric Learning and Psychometric Testing},
  author={<b>Yin, Lu</b> and Menkovski, Vlado and Pechenizkiy, Mykola},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={154--169},
  year={2020},
  organization={Springer},
  html={https://arxiv.org/pdf/2004.06353.pdf},
  pdf={Knowledge Elicitation using Deep Metric Learning and Psychometric Testing.pdf},
}

